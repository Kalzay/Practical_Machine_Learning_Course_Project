---
title: "Machine Learning - Course Project"
author: "Jonathan Hall"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

A recent trend among tech and/or fitness enthusiasts is to quantify how *much* of a particular activity they do, however, they rarely quantify how *well* they do it. In this project we aim to use data from accelerometers attached to the belt, forearm, arm and dumbell of 6 participants while they complete repetitions of bicep curls with dumb bells to predict whether the participant was doing it correctly or not. This is identified in the data by the `classe` variable: A is performing the curls correctly, whereas the rest represent incorrect curls.

The data and accompanying study used can be found here: [Qualitative Activity Recognition of Weight Lifting Exercises](http://web.archive.org/web/20161217164008/http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201)

# Packages and Seed setting

```{r results="hide", warning=FALSE, message=FALSE}
# Libraries
library(caret)
library(ggplot2)
library(randomForest)
library(parallel)
library(doParallel)
set.seed(17)
```

# Exploratory Data Analysis and Feature Selection

## Importing Data

First we import our data. We set the testing set to be a *validation* set as we plan to split our training data into training and testing sets. We also convert the `classe` variable into a factor variable.

```{r}
training_import <- read.csv('./pml-training.csv')
validating <- read.csv('./pml-testing.csv')
# classe as factor variable
training_import$classe <- as.factor(training_import$classe)
```

We split our training data into training and testing sets. The testing set will be used to test the model accuracy once we have built it before applying it to the validating set.

```{r}
inTrain <- createDataPartition(y=training_import$classe,p=0.75, list=FALSE)
# subset spam data to training
training <- training_import[inTrain,]
# subset spam data (the rest) to test
testing <- training_import[-inTrain,]
```

## Removing Zero Covariates

A first step we can take to reducing the amount of features we use in our model is to remove zero covariates. We use the `nearZeroVar` function identify features with near zero variance and then remove them from our training dataset. This is based on the `nzv` column having a value of `TRUE`.

```{r, cache=TRUE}
ZV <- nearZeroVar(training, saveMetrics = TRUE)
# The first six Near Zero Variance predictors
head(ZV[ZV$nzv==TRUE,])
remove <- ZV[,4] # Logical vector for removing NZV predictors
# Remove these predictors from the training dataset
training <- training[,remove==FALSE]
```

## Remove useless features

Taking a look at the first 5 variables in our dataset we can see the variables `X`, `user_name`, `raw_timestamp_part_1`, `raw_timestamp_part_2`, `cvtd_timestamp`. 

```{r}
head(training)[1:5]
```


All of the variables can be removed as they will not be predictive; `X` simply enumerates each observation; `user_name` is the name of the participant; and the remaining three are timestamps of the observations.

```{r}
# Remove variables X, user_name and time stamp (character) - won't be needed
training <- training[,-c(1:5)]
```

There are also many variables which contain a lot of missing values.

```{r}
subset(colMeans(is.na(training)),colMeans(is.na(training))>0)[1:4]
```

These variables are statistics of others: max, min, amplitude etc. They add no new information to the data so we may also remove them.

```{r}
training <- training[,colSums(is.na(training))==0]
```

## Checking our features with Principal Component Analysis

We can use the `prcomp` function to apply Principal Component Anaylsis to see how much variance is explained by the first few Principal Components.

```{r}
pr <- prcomp(training[,-54], scale=TRUE)
data.frame(summary(pr)$importance)[1:5]
```

We can see by looking at the Cumulative Proportion that we do require a fair amount of features - the Cumulative Proportion does not increase very quickly. Let's plot the first two Principal Components and see if we think our model will be able to do a good job of separating the data into the `classe` variable.

```{r}
qplot(x=PC1, y=PC2, data=data.frame(pr$x))
```

We can clearly see 5 distinct groups. This gives us confidence that model fitting should work well.

# Model Fitting

We will train a Random Forest model on our training dataset and then check for its accuracy on our testing dataset. Note I am using parallel processing here to better utilise my computers hardware and allow for quicker speeds in training the model.

```{r, cache=TRUE}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

fitControl <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE)

fit <- train(classe ~ ., method="rf",data=training,trControl = fitControl)

stopCluster(cluster)
registerDoSEQ()
```

Checking the Confusion Matrix, we can see that our model seems to work very well with a close to 100% accuracy.

```{r}
confusionMatrix(fit)
```

We use our testing dataset, which was created as a partition of the training dataset, to gain more insight into how accurate our model is.

```{r}
# Use testing dataset to verify
pred <- predict(fit,newdata=testing)
testing$predRight <- pred==testing$classe
# What percentage are correct predictions?
mean(testing$predRight==TRUE)
# Tabulate the predictions
table(pred, testing$classe)
```

Again, we see that our model is extremely accurate. The final step is to apply this model to the validating dataset to attain our answers to the quiz. I have witheld the output of this code to follow the Honour Code we all comply to in taking this course.

```{r, results='hide'}
# Apply model to validating set to answer quiz questions
predict(fit, validating)
```

